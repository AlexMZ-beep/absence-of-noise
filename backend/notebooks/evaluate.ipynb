{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()"
   ],
   "id": "d3905feb11e9c2d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "test_data = pd.read_csv('../../rzd_data/qna.csv')"
   ],
   "id": "47521f354e59a366",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import random\n",
    "from typing import Dict, List\n",
    "\n",
    "\n",
    "def create_examples(df) -> List[Dict[str, str]]:\n",
    "    examples = []\n",
    "    for i, row in df.iterrows():\n",
    "        examples.append({\n",
    "            \"question\": row['Вопрос'],\n",
    "            \"answer\": row['Ответ'],\n",
    "        })\n",
    "\n",
    "    return examples"
   ],
   "id": "c8240d6918f7a39b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "dataset_name = \"rzd_22\"\n",
    "dataset = client.read_dataset(dataset_name=dataset_name)"
   ],
   "id": "d522d291024664b1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "examples = create_examples(test_data)\n",
    "\n",
    "inputs, outputs = zip(\n",
    "    *[({\"question\": row[\"question\"]}, row) for row in examples]\n",
    ")"
   ],
   "id": "cbb9c2de9cce2d29",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "client.create_examples(inputs=inputs, outputs=outputs, dataset_id=dataset.id)",
   "id": "e706d056c2020aac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "def predict(_inputs: dict) -> dict:\n",
    "    response = requests.post('http://0.0.0.0:8080/predict', json=_inputs)\n",
    "    return response.json()"
   ],
   "id": "f2bc1f12563eb363",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class EvalResponse(BaseModel):\n",
    "    reason: str = Field(..., description=\"Причина выставления оценки\")\n",
    "    score: int = Field(..., description=\"Оценка ответа поданного на проверку\")"
   ],
   "id": "174ea8e42df570be",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import httpx\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    ")"
   ],
   "id": "100302a71bed7eec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "eval_system = \"\"\"\\\n",
    "Вы учитель, который проверяет викторину.\n",
    "\n",
    "Вам будут предоставлены ВОПРОС, ПРАВИЛЬНЫЙ ОТВЕТ и ОТВЕТ УЧЕНИКА.\n",
    "\n",
    "Вот критерии оценки:\n",
    "\n",
    "(1) Оценивайте ответы учащихся ТОЛЬКО на основе их фактической точности относительно правильного ответа.\n",
    "\n",
    "(2) Убедитесь, что ответ ученика не содержит противоречивых утверждений.\n",
    "\n",
    "(3) Допустимо, если ответ ученика содержит больше информации, чем правильный ответ, при условии, что она фактически точна относительно правильного ответа.\n",
    "\n",
    "Оценка:\n",
    "\n",
    "Оценка 1 означает, что ответ ученика соответствует всем критериям. Это самая высокая (лучшая) оценка.\n",
    "\n",
    "Оценка 0 означает, что ответ ученика не соответствует всем критериям. Это самая низкая оценка, которую вы можете поставить.\n",
    "\n",
    "Объясните свое рассуждение пошагово, чтобы убедиться, что ваши выводы корректны.\n",
    "\n",
    "Избегайте просто указывать правильный ответ в самом начале.\"\"\"\n",
    "\n",
    "eval_human = \"\"\"\\\n",
    "QUESTION: {question}\n",
    "GROUND TRUTH ANSWER: {correct_answer}\n",
    "STUDENT ANSWER: {student_answer}\"\"\""
   ],
   "id": "cc76dbf201ed266",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "eval_prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', eval_system),\n",
    "    ('human', eval_human),\n",
    "])"
   ],
   "id": "c5fd5bb7ae7bcdcd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def exact_match_class_1(run, example):\n",
    "    return {\"score\": run.outputs[\"class_1\"] == example.outputs[\"class_1\"]}\n",
    "\n",
    "\n",
    "def exact_match_class_2(run, example):\n",
    "    return {\"score\": run.outputs[\"class_2\"] == example.outputs[\"class_2\"]}\n",
    "\n",
    "\n",
    "def exact_match_bz_answer(run, example):\n",
    "    return {\"score\": run.outputs[\"answer_from_bz\"] == example.outputs[\"answer_from_bz\"]}\n",
    "\n",
    "\n",
    "def check_bz_answer_in_docs(run, example):\n",
    "    answer_from_bz = example.outputs[\"answer_from_bz\"]\n",
    "    return {\"score\": answer_from_bz in run.outputs[\"docs\"]}\n",
    "\n",
    "\n",
    "def middle_num_docs(run, example):\n",
    "    return {\"score\": run.outputs[\"total_docs\"]}\n",
    "\n",
    "\n",
    "def check_answer_correctness(run, example):\n",
    "    model_with_so = model.with_structured_output(EvalResponse)\n",
    "    evaluate_chain = eval_prompt | model_with_so\n",
    "    result = evaluate_chain.invoke({\n",
    "        \"student_answer\": run.outputs[\"answer\"],\n",
    "        \"correct_answer\": example.outputs[\"answer\"],\n",
    "        \"question\": example.inputs[\"question\"],\n",
    "    })\n",
    "    return {\"score\": result.score, \"description\": result.reason, \"key\": \"answer_score\"}"
   ],
   "id": "4ae5c4fc37db07e6",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from langsmith import evaluate\n",
    "\n",
    "evaluate(\n",
    "    predict,\n",
    "    data=dataset_name,\n",
    "    evaluators=[check_answer_correctness],\n",
    "    metadata={\"revision_id\": \"v.0.1.0\"},\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
